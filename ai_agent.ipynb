{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46eb0b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# LangChain + vector DB + embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Gemini SDK\n",
    "import google.generativeai as genai\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb55a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"ANONYMIZED_TELEMETRY\"] = \"false\"\n",
    "# os.environ[\"CHROMA_TELEMETRY\"] = \"false\"\n",
    "# os.environ[\"OTEL_SDK_DISABLED\"] = \"true\"\n",
    "# os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14cd1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable tracing\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"AI Agent using LangGraph\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaaef6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGSMITH_API_KEY=os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45bc3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE DEFINITION\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    answer: str\n",
    "    reflection: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f3077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMENT LOADING\n",
    "\n",
    "def load_documents(folder=\"data\"):\n",
    "    folder = Path(folder)\n",
    "\n",
    "    docs = []\n",
    "    for file in folder.glob(\"*.txt\"):\n",
    "        text = file.read_text(encoding=\"utf-8\")\n",
    "        print(f\"[LOAD] Loaded {file.name}\")\n",
    "        docs.append(text)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD VECTOR DATABASE\n",
    "\n",
    "def build_vectordb(texts):\n",
    "    splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "    chunks = splitter.split_text(\"\\n\\n\".join(texts))\n",
    "\n",
    "    print(f\"[VDB] Created {len(chunks)} text chunks\")\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    vectordb = Chroma.from_texts(chunks, embedding=embeddings)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD GEMINI\n",
    "def load_gemini():\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Please set GEMINI_API_KEY environment variable.\")\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"Using Gemini 2.0 Model\")\n",
    "    return genai.GenerativeModel(\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c1cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan Node\n",
    "def plan_node(state: AgentState):\n",
    "    q = state[\"question\"]\n",
    "    qwords = [\"what\", \"why\", \"how\", \"benefit\", \"explain\", \"Tell me about\"]\n",
    "\n",
    "    need = any(k in q.lower() for k in qwords)\n",
    "    print(\"\\n[PLAN] retrieval needed?\", need)\n",
    "\n",
    "    return {\"context\": \"RETRIEVE\" if need else \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Node\n",
    "def retrieve_node(state: AgentState):\n",
    "    print(\"[RETRIEVE] retrieving chunks from vector DB...\")\n",
    "    results = vectordb.similarity_search(state[\"question\"], k=2)\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join([r.page_content for r in results]) if results else \"\"\n",
    "    return {\"context\": context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Node\n",
    "def answer_node(state: AgentState):\n",
    "    q = state[\"question\"]\n",
    "    c = state[\"context\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Use ONLY the context to answer the question.\n",
    "\n",
    "Context:\n",
    "{c}\n",
    "\n",
    "Question:\n",
    "{q}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = GEMINI.generate_content(prompt)\n",
    "    answer = response.text\n",
    "\n",
    "    print(\"[ANSWER]\", answer[:80], \"...\")\n",
    "    return {\"answer\": answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "392a9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflect Node\n",
    "def reflect_node(state: AgentState):\n",
    "    q = state[\"question\"]\n",
    "    a = state[\"answer\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Rate the relevance of the answer from 0 to 10. Then explain briefly.\n",
    "\n",
    "Question: {q}\n",
    "Answer: {a}\n",
    "\n",
    "Format:\n",
    "Score: <0-10>\n",
    "Reason: <one sentence>\n",
    "\"\"\"\n",
    "\n",
    "    response = GEMINI.generate_content(prompt)\n",
    "    review = response.text\n",
    "\n",
    "    print(\"[REFLECT]\", review[:80].replace(\"\\n\", \" \"), \"...\")\n",
    "    return {\"reflection\": review}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5550779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD LANGGRAPH\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"plan\", plan_node)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"answer\", answer_node)\n",
    "graph.add_node(\"reflect\", reflect_node)\n",
    "\n",
    "graph.set_entry_point(\"plan\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"plan\",\n",
    "    lambda s: \"retrieve\" if s[\"context\"] == \"RETRIEVE\" else \"answer\",\n",
    "    {\"retrieve\": \"retrieve\", \"answer\": \"answer\"},\n",
    ")\n",
    "\n",
    "graph.add_edge(\"retrieve\", \"answer\")\n",
    "graph.add_edge(\"answer\", \"reflect\")\n",
    "graph.add_edge(\"reflect\", END)\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38e2e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN RAG FUNCTION\n",
    "def ask(question: str):\n",
    "    input_state = {\n",
    "        \"question\": question,\n",
    "        \"context\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"reflection\": \"\",\n",
    "    }\n",
    "\n",
    "    result = app.invoke(input_state)\n",
    "    print(\"\\n===== FINAL ANSWER =====\\n\")\n",
    "    print(result[\"answer\"])\n",
    "\n",
    "    print(\"\\n===== REFLECTION =====\\n\")\n",
    "    print(result[\"reflection\"])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "785e9957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 741, which is longer than the specified 400\n",
      "Created a chunk of size 799, which is longer than the specified 400\n",
      "Created a chunk of size 785, which is longer than the specified 400\n",
      "Created a chunk of size 753, which is longer than the specified 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Gemini 2.0 Model\n",
      "[LOAD] Loaded ai.txt\n",
      "[LOAD] Loaded data_science.txt\n",
      "[LOAD] Loaded deep_learning.txt\n",
      "[LOAD] Loaded machine_learning.txt\n",
      "[LOAD] Loaded python_programing.txt\n",
      "[VDB] Created 5 text chunks\n",
      "\n",
      "RAG Agent ready — type a question or 'exit' to quit.\n",
      "\n",
      "\n",
      "[PLAN] retrieval needed? True\n",
      "[RETRIEVE] retrieving chunks from vector DB...\n",
      "[ANSWER] The development of machines capable of performing tasks that typically require h ...\n",
      "[REFLECT] Score: 10 Reason: The answer provides a concise and accurate definition of AI.  ...\n",
      "\n",
      "===== FINAL ANSWER =====\n",
      "\n",
      "The development of machines capable of performing tasks that typically require human intelligence.\n",
      "\n",
      "\n",
      "===== REFLECTION =====\n",
      "\n",
      "Score: 10\n",
      "Reason: The answer provides a concise and accurate definition of AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN (interactive mode)\n",
    "if __name__ == \"__main__\":\n",
    "    GEMINI = load_gemini()\n",
    "    docs = load_documents()\n",
    "    vectordb = build_vectordb(docs)\n",
    "\n",
    "    print(\"\\nRAG Agent ready — type a question or 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        q = input(\"Ask: \").strip()\n",
    "        if q.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        ask(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38920fa4",
   "metadata": {},
   "source": [
    "\n",
    "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
    "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101d1ad",
   "metadata": {},
   "source": [
    "c:\\Users\\abhin\\FSDS\\task 3\\ai_env\\Lib\\site-packages\\pydantic\\v1\\main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
    "            id = uuid7()\n",
    "Future versions will require UUID v7.\n",
    "  input_data = validator(cls_, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb88de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8af916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2023ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673afc02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f13ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
